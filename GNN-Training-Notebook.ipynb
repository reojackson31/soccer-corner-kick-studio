{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSY670 Social Media Analytics: Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2: Training Graph Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = 'data/events'\n",
    "frames_path = 'data/frames'\n",
    "\n",
    "events = pd.DataFrame()\n",
    "frames = pd.DataFrame()\n",
    "\n",
    "def append_json_to_df(folder_path, dataframe):\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.json'):\n",
    "            try:\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                data = pd.read_json(file_path)\n",
    "                if folder_path=='data/events':\n",
    "                    data['team_name'] = data['team'].apply(lambda x: x['name'] if isinstance(x, dict) and 'name' in x else np.nan)\n",
    "                    unique_teams = data['team_name'].unique().tolist()\n",
    "                    data['team1'] = unique_teams[0]\n",
    "                    data['team2'] = unique_teams[1]\n",
    "                    data.drop(['team_name'],axis=1,inplace=True)\n",
    "                dataframe = pd.concat([dataframe, data], ignore_index=True)\n",
    "                dataframe.reset_index(drop=True, inplace=True)\n",
    "            except:\n",
    "                continue\n",
    "    return dataframe\n",
    "\n",
    "events = append_json_to_df(events_path, events)\n",
    "frames = append_json_to_df(frames_path, frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469072, 401328)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(events), len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "events['pass_type'] = events['pass'].apply(lambda x: x['type']['name'] if isinstance(x, dict) and 'type' in x and 'name' in x['type'] else np.nan)\n",
    "events['pass_shot_assist'] = events['pass'].apply(lambda x: x['shot_assist'] if isinstance(x, dict) and 'shot_assist' in x else False)\n",
    "events['attack_team'] = events['team'].apply(lambda x: x['name'] if isinstance(x, dict) and 'name' in x else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping from EA FC dataset to add player and team attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "events['defense_team'] = events['team2']\n",
    "events.loc[events['attack_team']==events['team2'],'defense_team'] = events['team1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_data = pd.read_csv('data/male_teams.csv')\n",
    "fifa_data = fifa_data[['team_name', 'attack', 'defence']]\n",
    "\n",
    "mean_attack = fifa_data['attack'].mean()\n",
    "mean_defense = fifa_data['defence'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the network with the corner frame and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = events[events['pass_type']=='Corner']\n",
    "corners_wframes = pd.merge(corners, frames, left_on='id', right_on='event_uuid', how='inner')\n",
    "\n",
    "network_data = corners_wframes[['freeze_frame', 'event_uuid', 'pass_shot_assist', 'attack_team', 'defense_team']]\n",
    "network_data['id'] = network_data.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data = pd.merge(network_data, fifa_data[['team_name','attack']], left_on='attack_team', right_on='team_name',how='left')\n",
    "network_data['attack'].fillna(mean_attack, inplace=True)\n",
    "network_data.drop(['team_name'], axis=1, inplace=True)\n",
    "network_data.drop_duplicates(subset=['id'], inplace=True)\n",
    "network_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "network_data = pd.merge(network_data, fifa_data[['team_name','defence']], left_on='defense_team', right_on='team_name',how='left')\n",
    "network_data['defence'].fillna(mean_defense, inplace=True)\n",
    "network_data.drop_duplicates(subset=['id'], inplace=True)\n",
    "network_data.drop(['attack_team','defense_team','team_name','id'], axis=1, inplace=True)\n",
    "network_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pass_shot_assist\n",
       "False    558\n",
       "True     141\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_data['pass_shot_assist'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stratified split on Target variable (Pass Shot Assist)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=123)\n",
    "for train_index, test_index in split.split(network_data, network_data[\"pass_shot_assist\"]):\n",
    "    train = network_data.loc[train_index]\n",
    "    test = network_data.loc[test_index]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the data to pytorch geometric dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(row):\n",
    "    players = row['freeze_frame']\n",
    "    attack_rating = row['attack']\n",
    "    defense_rating = row['defence']\n",
    "\n",
    "    features = [\n",
    "        [player['location'][0], player['location'][1],\n",
    "         attack_rating if player['teammate'] else defense_rating]\n",
    "        for player in players\n",
    "    ]\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float)\n",
    "    \n",
    "    edges = []\n",
    "    for i in range(len(players)):\n",
    "        for j in range(i + 1, len(players)):\n",
    "            if players[i]['teammate'] == players[j]['teammate']:\n",
    "                edges.append([i, j])\n",
    "                edges.append([j, i])\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    target = torch.tensor([int(row['pass_shot_assist'])], dtype=torch.long)\n",
    "    \n",
    "    graph_data = Data(x=features_tensor, edge_index=edge_index, y=target)\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = [create_graph_data(row) for index, row in train.iterrows()]\n",
    "test_graph = [create_graph_data(row) for index, row in test.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 489\n",
      "Number of test graphs: 210\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12345)\n",
    "\n",
    "train_dataset = train_graph.copy()\n",
    "test_dataset = test_graph.copy()\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[17, 3], edge_index=[2, 128], y=[1])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[1174, 3], edge_index=[2, 10016], y=[64], batch=[1174], ptr=[65])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[1166, 3], edge_index=[2, 9808], y=[64], batch=[1166], ptr=[65])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[1162, 3], edge_index=[2, 9718], y=[64], batch=[1162], ptr=[65])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[1181, 3], edge_index=[2, 10054], y=[64], batch=[1181], ptr=[65])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[1151, 3], edge_index=[2, 9662], y=[64], batch=[1151], ptr=[65])\n",
      "\n",
      "Step 6:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[1189, 3], edge_index=[2, 10216], y=[64], batch=[1189], ptr=[65])\n",
      "\n",
      "Step 7:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[1171, 3], edge_index=[2, 9912], y=[64], batch=[1171], ptr=[65])\n",
      "\n",
      "Step 8:\n",
      "=======\n",
      "Number of graphs in the current batch: 41\n",
      "DataBatch(x=[741, 3], edge_index=[2, 6220], y=[41], batch=[741], ptr=[42])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64, num_node_features=3, num_classes=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed\n",
      "Train Accuracy: 0.7975\n",
      "Test Accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  \n",
    "         out = model(data.x, data.edge_index, data.batch) \n",
    "         loss = criterion(out, data.y)  \n",
    "         loss.backward()  \n",
    "         optimizer.step()  \n",
    "         optimizer.zero_grad()\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  \n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         pred = out.argmax(dim=1)  \n",
    "         correct += int((pred == data.y).sum())  \n",
    "     return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 1000):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "\n",
    "print(\"Model training completed\")\n",
    "print(f'Train Accuracy: {train_acc:.4f}')\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(3, 64)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (conv3): GCNConv(64, 64)\n",
       "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'gnn_model.pth')\n",
    "model.load_state_dict(torch.load('gnn_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of leading to a shot: 0.18485227227210999\n"
     ]
    }
   ],
   "source": [
    "def predict(data):\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x.unsqueeze(0), data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long))\n",
    "        prob = F.softmax(out.squeeze(), dim=0)\n",
    "        return prob\n",
    "\n",
    "# Make prediction\n",
    "probability = predict(data)\n",
    "print(\"Probability of leading to a shot:\", probability[1].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
